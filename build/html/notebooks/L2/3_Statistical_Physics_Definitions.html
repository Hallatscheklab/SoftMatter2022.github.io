<!doctype html>
<html class="no-js">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><link rel="index" title="Index" href="../../genindex.html" /><link rel="search" title="Search" href="../../search.html" /><link rel="prev" title="Thermodynamics and Statistical Physics Revisited" href="../L1/2_Thermodynamics_Statistics.html" />

    <meta name="generator" content="sphinx-4.2.0, furo 2021.09.08"/>
        <title>Statistical Physics Definitions - Soft Matter Lecture 21 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo.css?digest=c7c65a82b42f6b978e58466c1e9ef2509836d916" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo-extensions.css?digest=16fb25fabf47304eee183a5e9af80b1ba98259b1" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  body[data-theme="dark"] {
    --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
  }
  @media (prefers-color-scheme: dark) {
    body:not([data-theme="light"]) {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
  }
</style></head>
  <body>
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round">
      <path stroke="none" d="M0 0h24v24H0z" />
      <line x1="4" y1="6" x2="20" y2="6" />
      <line x1="10" y1="12" x2="20" y2="12" />
      <line x1="6" y1="18" x2="20" y2="18" />
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../../index.html"><div class="brand">Soft Matter Lecture 21 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand centered" href="../../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../../_static/mona_logo.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">Soft Matter Lecture 21 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../../search.html" role="search">
  <input class="sidebar-search" placeholder=Search name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Course Information:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../course-info/info.html">Course Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../course-info/instructors.html">Instructors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../course-info/resources.html">Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../course-info/exam.html">Exercises and Exam</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Lecture 1:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../L1/1_introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../L1/2_Thermodynamics_Statistics.html">Thermodynamics and Statistical Physics Revisited</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Lecture 2:</span></p>
<ul class="current">
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">Statistical Physics Definitions</a></li>
</ul>

</div>
</div>
      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <div class="content-icon-container">
          <div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
</style>
<div class="admonition note">
<p>This page was generated from <cite>notebooks/L2/3_Statistical_Physics_Definitions.ipynb</cite>.
<span class="raw-html"><br/><a href="https://mybinder.org/v2/gh/fcichos/SoftMatterPhysics/main?urlpath=tree/source/notebooks/L2/3_Statistical_Physics_Definitions.ipynb"><img alt="Binder badge" src="https://img.shields.io/badge/launch-full%20binder-red.svg" style="vertical-align:text-bottom"/></a></span></p>
</div>
<div class="section" id="Statistical-Physics-Definitions">
<h1>Statistical Physics Definitions<a class="headerlink" href="#Statistical-Physics-Definitions" title="Permalink to this headline">¶</a></h1>
<p>The field of statitical physics uses approaches of statistics and probability theory to address physical problems. It considers large populations and derives expressions for the ensemble (or the macrostate) of a system from the microscopic states in the system.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">factorial</span> <span class="k">as</span> <span class="n">fac</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = 'retina'

<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">'font.size'</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span>
                     <span class="s1">'axes.titlesize'</span><span class="p">:</span> <span class="mi">18</span><span class="p">,</span>
                     <span class="s1">'axes.labelsize'</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
                     <span class="s1">'axes.labelpad'</span><span class="p">:</span> <span class="mi">14</span><span class="p">,</span>
                     <span class="s1">'lines.linewidth'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
                     <span class="s1">'lines.markersize'</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
                     <span class="s1">'xtick.labelsize'</span> <span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
                     <span class="s1">'ytick.labelsize'</span> <span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
                     <span class="s1">'xtick.top'</span> <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                     <span class="s1">'xtick.direction'</span> <span class="p">:</span> <span class="s1">'in'</span><span class="p">,</span>
                     <span class="s1">'ytick.right'</span> <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                     <span class="s1">'ytick.direction'</span> <span class="p">:</span> <span class="s1">'in'</span><span class="p">,})</span>
</pre></div>
</div>
</div>
<div class="section" id="Entropy">
<h2>Entropy<a class="headerlink" href="#Entropy" title="Permalink to this headline">¶</a></h2>
<div class="section" id="Entropy-Definition-by-Boltzmann">
<h3>Entropy Definition by Boltzmann<a class="headerlink" href="#Entropy-Definition-by-Boltzmann" title="Permalink to this headline">¶</a></h3>
<p>The term of entropy becomes very important in that context. It measures the number of different ways a system can be rearranged to yield the same macrostate. It is, thus, an indicator for the microscopic degeneracy of a macrostate. In this context the definition of entropy by <strong>Boltzmann</strong> is well known, i.e.,</p>
<p><span class="math">\begin{equation}
S=k_\mathrm{B} \ln(W)
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(W\)</span> is the number of microstates corresponding to a system’s macrostate with and energy <span class="math notranslate nohighlight">\(E\)</span>. Here <span class="math notranslate nohighlight">\(k_\mathrm{B}=1.38064852 × 10^{-23} {\rm m^2\, kg\, s^{-2}\, K^{-1}}\)</span> is the Boltzmann constant. Below are two examples of how to use the formula for the calculation of the entropy.</p>
<div class="admonition note">
<p class="admonition-title"><strong>Example: Entropy of an N letter word</strong></p>
<p align="center"><p><img alt="letters" class="bg-primary mb-1 no-scaled-link" src="../../_images/letters.png" style="width: 300px;"/></p>
<p><p>Consider the number of states <span class="math notranslate nohighlight">\(W\)</span> of a word with <span class="math notranslate nohighlight">\(N\)</span> letters of <span class="math notranslate nohighlight">\(M\)</span> different characters. The letter can be arranged in</p>
<p><span class="math">\begin{equation}
W=M^N
\end{equation}</span></p>
<p>different ways such that the entropy is given by</p>
<p><span class="math">\begin{equation}
S=N\, k_\mathrm{B} \ln(M) = k_{\rm B}\sum_{1}^{N} \ln(M)
\end{equation}</span></p>
<p>which just tells that entropy is an additive quantity.</p>
</p></p></div>
<div class="admonition note">
<p class="admonition-title"><strong>Example: Arrangement of Molecules along a Chain</strong></p>
<p align="center"><p><img alt="binding" class="bg-primary mb-1 no-scaled-link" src="../../_images/binding.png" style="width: 500px;"/></p>
<p><p>We consider a linear molecules (perhaps a DNA) that has <span class="math notranslate nohighlight">\(N\)</span> binding sites for, e.g., proteins. <span class="math notranslate nohighlight">\(N_\mathrm{p}\)</span> sites are occupied with a protein where the binding energy is equal for each site. The number of different ways in which the <span class="math notranslate nohighlight">\(N_\mathrm{p}\)</span> proteins can be arranged on the <span class="math notranslate nohighlight">\(N\)</span> sites is given by the binomial coefficient</p>
<p><span class="math">\begin{equation}
W(N_\mathrm{p};N)=\frac{N!}{N_\mathrm{p}!(N-N_\mathrm{p})!}.
\end{equation}</span></p>
<p>Therefore, the entropy is given by</p>
<p><span class="math">\begin{equation}
S=k_\mathrm{B} \ln\left ( \frac{N!}{N_\mathrm{p}!(N-N_\mathrm{p})!}\right )
\end{equation}</span></p>
<p>which can be further simplified using the identity</p>
<p><span class="math">\begin{equation}
\ln(N!)=\sum_{n=1}^{N}\ln(n)
\end{equation}</span></p>
<p>and the <em>Stirling approximation</em></p>
<p><span class="math">\begin{equation}
\sum_{n=1}^{N}\ln(n)\approx \int_1^{N}\ln(x)\,\mathrm{d}x\approx N\ln(N)-N.
\end{equation}</span></p>
<p>This, finally, leads to</p>
<p><span class="math">\begin{equation}
S=-k_\mathrm{B} N [c \ln(c)+(1-c)\ln(1-c)]
\end{equation}</span></p>
<p>with <span class="math notranslate nohighlight">\(c=N_\mathrm{p}/N\)</span> being the mean occupation of each site or the probability to find a state occupied.</p>
</p></p></div>
<p>Below you just find some Python code calculating the entropy as a function of “concentration” using the Stirling approximation and the original formula. You also recognize there, that the Stirling formula is not yet very good, since <span class="math notranslate nohighlight">\(N=100\)</span>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-3</span>
<span class="n">c</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">epsilon</span><span class="p">,</span><span class="mi">1</span><span class="o">-</span><span class="n">epsilon</span><span class="p">,</span><span class="mi">100</span><span class="p">);</span><span class="n">N</span><span class="o">=</span><span class="mi">100</span>
<span class="n">Np</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">N</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">S</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">fac</span><span class="p">(</span><span class="n">N</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">fac</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="n">fac</span><span class="p">(</span><span class="n">N</span><span class="o">-</span><span class="n">x</span><span class="p">))))</span>

</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="o">-</span><span class="p">(</span><span class="n">c</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">c</span><span class="p">)</span><span class="o">+</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">c</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">c</span><span class="p">)),</span><span class="s1">'r-'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">"Stirling"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Np</span><span class="o">/</span><span class="n">N</span><span class="p">,</span><span class="n">S</span><span class="p">(</span><span class="n">Np</span><span class="p">)</span><span class="o">/</span><span class="n">N</span><span class="p">,</span><span class="s1">'k--'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">"Original"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"$c$"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">"$S$ [$k_\mathrm</span><span class="si">{B}</span><span class="s2">$]"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_L2_3_Statistical_Physics_Definitions_7_0.png" class="no-scaled-link" src="../../_images/notebooks_L2_3_Statistical_Physics_Definitions_7_0.png" style="width: 408px; height: 280px;"/>
</div>
</div>
</div>
<div class="section" id="Shannon-Entropy">
<h3>Shannon Entropy<a class="headerlink" href="#Shannon-Entropy" title="Permalink to this headline">¶</a></h3>
<p>A different access to entropy comes from the field of information theory and has been divised by Claude Shannon. Information theory is trying to mathematically assess the information content of measurement facing uncertainty. It will turn out further below that this alternative description results in the Boltzmann distribution and effectively amounts to making a best guess about the probability distribution given some limited knowledge about the system such as the average energy.</p>
<p>The Shannon entropy is defined by</p>
<p><span class="math">\begin{equation}
S\left(p_{1}, p_{2}, \ldots, p_{N}\right)=S\left(\left\{p_{i}\right\}\right)=-\sum_{i=1}^{N} p_{i} \ln p_{i}
\end{equation}</span></p>
<p>and relates to its thermodynamic version, the Gibbs entropy</p>
<p><span class="math">\begin{equation}
S\left(\left\{p_{i}\right\}\right)=-k_B\sum_{i=1}^{N} p_{i} \ln p_{i}
\end{equation}</span></p>
<p>where the <span class="math notranslate nohighlight">\(p_i\)</span> is the probability of the <span class="math notranslate nohighlight">\(i\)</span>th microstate (or outcome). The example below, will show that if only the normalization of the probability is known, maximization of the Shannon entropy will directly lead to an equal probabiliyt of events (equal distribution). Later, we see that similar calculations can be done to yields the Boltzmann distribution.</p>
<div class="admonition note">
<p class="admonition-title"><strong>Example: Equal Distribution</strong></p>
<p>To figure out that the Shannon entropy is indeed delivering some useful measure, we will have a look at a measurement which has <span class="math notranslate nohighlight">\(N\)</span> outcomes (e.g. rolling a dice). We of course know that in this case all number of the dice have equal probability, but we can test this by maximizing the Shannon entropy as required by our thermodynamic considerations earlier.</p>
<p>To do so, we use the technique of Lagrangian multipliers, which allows us to set a constraint, while maximizing the entropy. This contraint is for this example, that</p>
<p><span class="math">\begin{equation}
\sum_{i}^{N} p_{i}=1
\end{equation}</span></p>
<p>i.e. that the probability is normalized to 1. With this constraint we maximize the entropy by adding an additional term with the contrains multilied by the Lagrangien multiplier <span class="math notranslate nohighlight">\(\lambda\)</span></p>
<p><span class="math">\begin{equation}
S^{\prime}=-\sum_{i} p_{i} \ln p_{i}-\lambda\left(\sum_{i} p_{i}-1\right)
\end{equation}</span></p>
<p>We see, that if the probability is normalized to 1, we do not change the entropy. Our procedure is to find that set of probabilities <span class="math notranslate nohighlight">\(p_i\)</span> which maximize this augmented entropy function.</p>
<p>The derivative of the augmented entropy with respect to <span class="math notranslate nohighlight">\(\lambda\)</span> yields the normalization condition, i.e.</p>
<p><span class="math">\begin{equation}
\frac{\partial S^{\prime}}{\partial \lambda}=0=-\left(\sum_{i} p_{i}-1\right)
\end{equation}</span></p>
<p>Differentiation with respect to the probabilities yields</p>
<p><span class="math">\begin{equation}
\frac{\partial S^{\prime}}{\partial p_{i}}=0=-\ln p_{i}-1-\lambda
\end{equation}</span></p>
<p>which directly gives</p>
<p><span class="math">\begin{equation}
p_{i}=\mathrm{e}^{-1-\lambda}
\end{equation}</span></p>
<p>Together with the normalization condition we therefore obtain</p>
<p><span class="math">\begin{equation}
\sum_{i=1}^{N} \mathrm{e}^{-1-\lambda}=1
\end{equation}</span></p>
<p>and since the exponent does not depend on <span class="math notranslate nohighlight">\(i\)</span> we find</p>
<p><span class="math">\begin{equation}
\mathrm{e}^{-1-\lambda}=\frac{1}{N}
\end{equation}</span></p>
<p>or</p>
<p><span class="math">\begin{equation}
p_{i}=\frac{1}{N}
\end{equation}</span></p>
<p>which is the expected equal probability of finding one of the <span class="math notranslate nohighlight">\(N\)</span> outcomes.</p>
</div>
</div>
</div>
<div class="section" id="Boltzmann-Distribution">
<h2>Boltzmann Distribution<a class="headerlink" href="#Boltzmann-Distribution" title="Permalink to this headline">¶</a></h2>
<p>Our previous consideration of the state functions has shown, that thermal equibrium is associated with a minimum in free energy. As the free energy consists of internal energy <span class="math notranslate nohighlight">\(U\)</span> (or enthalpy <span class="math notranslate nohighlight">\(H\)</span>) and an entropic term (<span class="math notranslate nohighlight">\(-TS\)</span>), we may understand this minimization as a competition between the minimization of the internal energy and a maximization of the entropy (since it is <span class="math notranslate nohighlight">\(-TS\)</span>). The figure below illustrates this competition for a gas in the gravity field.</p>
<p><img alt="entropy" class="bg-primary mb-1 no-scaled-link" src="../../_images/entropy_comp.png" style="width: 500px;"/></p>
<p>The internal energy minimization yields just a condensed layer at the bottom of the container, while the entropy maximization will try to spread the particles evenly (middle picture). The compromise of both at finite temperature is given by the <strong>barometric height formula</strong>, i.e.,</p>
<p><span class="math">\begin{equation}
p(z)=p_0\exp\left ( -\frac{m g z}{k_\mathrm{B} T}\right ),
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(p(z)\)</span> is the probability to find a particle at height <span class="math notranslate nohighlight">\(z\)</span>, <span class="math notranslate nohighlight">\(m\)</span> is the mass of a particle, <span class="math notranslate nohighlight">\(g\)</span> is the gravitational acceleration and <span class="math notranslate nohighlight">\(p_0\)</span> is a normalization constant. The result actually gives a hint at some very fundamental distribution, which always provides the free energy minimum in thermal equilibirum. This distribution is the Boltzmann distribution.</p>
<p>The Boltzmann distribution is an approach of statistical physics to describe a thermodynamic system in equilibrium. The idea is hereby to deliver probability distributions for the probability of all different microstates. Key distinguishing feature of different microstates is their energy <span class="math notranslate nohighlight">\(E_i\)</span> (that was so far neglected in the examples above), where <span class="math notranslate nohighlight">\(i\)</span> indicates the <span class="math notranslate nohighlight">\(i\)</span>th microstate.</p>
<p>The Boltzmann distribution tells us precisely the probability of finding a given microstate with energy <span class="math notranslate nohighlight">\(E_i\)</span>: If a particle is in equilibrium with its environment then the probability of finding the particle in state <span class="math notranslate nohighlight">\(i\)</span> with energy <span class="math notranslate nohighlight">\(E_i\)</span> is</p>
<p><span class="math">\begin{equation}
p(E_i)=\frac{1}{Z}\exp\left ( -\frac{E_i}{k_\mathrm{B} T}\right ).
\end{equation}</span></p>
<p>The normalization factor <span class="math notranslate nohighlight">\(1/Z\)</span> contains the so-called partition function <span class="math notranslate nohighlight">\(Z\)</span>:</p>
<p><span class="math">\begin{equation}
Z=\sum_{i}\exp\left ( -\frac{E_i}{k_\mathrm{B} T}\right )={\rm const}.
\end{equation}</span></p>
<p>It ensures that the total probablity to find a system in any of the states is</p>
<p><span class="math">\begin{equation}
\sum_i p(E_i)=1.
\end{equation}</span></p>
<div class="section" id="Mean-Energy">
<h3>Mean Energy<a class="headerlink" href="#Mean-Energy" title="Permalink to this headline">¶</a></h3>
<p>The Boltzmann distribution is useful to calculate also expectation values, for example, of the total energy of the system (the mean energy <span class="math notranslate nohighlight">\(\langle E\rangle\)</span>).</p>
<p>The mean is defined by: <span class="math">\begin{equation}
\langle E \rangle=\frac{1}{Z}\sum_{i=1}^{N}E_{i}\exp\left ( -\frac{E_i}{k_\mathrm{B} T}\right).
\end{equation}</span></p>
<p>Abbrevating <span class="math notranslate nohighlight">\(\beta=(k_\mathrm{B} T)^1\)</span> we find</p>
<p><span class="math">\begin{equation}
\langle E \rangle=\frac{1}{Z}\sum_{i=1}^{N}- \frac{\partial }{\partial \beta}\exp\left ( -\beta E_i\right),
\end{equation}</span></p>
<p>where the sum is nothing else than the derivative of the partition function</p>
<p><span class="math">\begin{equation}
\langle E \rangle=-\frac{1}{Z} \frac{\partial }{\partial \beta}Z
\end{equation}</span></p>
<p>or just</p>
<p><span class="math">\begin{equation}
\langle E \rangle=-\frac{\partial }{\partial \beta}\ln(Z).
\end{equation}</span></p>
</div>
<div class="section" id="Free-energy">
<h3>Free energy<a class="headerlink" href="#Free-energy" title="Permalink to this headline">¶</a></h3>
<p>Using the Gibbs entropy <span class="math notranslate nohighlight">\(S=-k_B\sum_{i=1}^{N} p_{i} \ln p_{i}\)</span> to find a relation between the free energy <span class="math notranslate nohighlight">\(F\)</span> (or <span class="math notranslate nohighlight">\(G\)</span>) and the partition function. Inserting the probability <span class="math notranslate nohighlight">\(p_i=Z^{1}\exp(-\beta E_i)\)</span> and doing some transformations yields</p>
<p><span class="math">\begin{equation}
S=k_B (\ln(Z)+\beta <e>)
\end{equation}</e></span></p>
<p>Using</p>
<p><span class="math">\begin{equation}
F=U-TS
\end{equation}</span></p>
<p>for the free energy, we can insert the above result for the entropy and obtain</p>
<p><span class="math">\begin{equation}
F=-k_\mathrm{B} T \ln(Z).
\end{equation}</span></p>
<p>(or <span class="math notranslate nohighlight">\(G\)</span> in the same way)</p>
<p>Note that this is the total energy and not the mean energy (internal, enthalpy or free energy) of the states in the system. The partition function thus allows us to calculate the free energy.</p>
</div>
<div class="section" id="Deriving-the-Boltzmann-Distribution">
<h3>Deriving the Boltzmann Distribution<a class="headerlink" href="#Deriving-the-Boltzmann-Distribution" title="Permalink to this headline">¶</a></h3>
<p>There are a number of ways to derive the Boltzmann distribution. We will have a quick look at a classical derivation of the Boltzmann distribution for a closed system, e.g. a system which is in contact with a reservoir as depicted below.</p>
<p><img alt="entropy_calc" class="bg-primary mb-1 no-scaled-link" src="../../_images/entropy_calc.png" style="width: 300px;"/></p>
<p>System (index <span class="math notranslate nohighlight">\(S\)</span>) and reservoir (index <span class="math notranslate nohighlight">\(R\)</span>) have total energy <span class="math notranslate nohighlight">\(E_{\rm tot}=E_{\rm R}+E_{\rm S}\)</span>. We assert now, that the probability to find the system in a specific microstate <span class="math notranslate nohighlight">\(p(E_s^i)\)</span> with the energy <span class="math notranslate nohighlight">\(E_s^i\)</span> is directly proportional to the number of states available to the reservoir, when the system is in that state. The ratio of the probability of two states is then equal to the ratio of the number of states of the reservoir, i.e.</p>
<p><span class="math">\begin{equation}
\frac{p(E_s^{(1)})}{p(E_s^{(2)}}=\frac{W_{\rm R}(E_{\rm tot}-E_{\rm S}^{(1)})}{W_{\rm R}(E_{\rm tot}-E_{\rm S}^{(2)})}
\end{equation}</span></p>
<p>Here, <span class="math notranslate nohighlight">\(W_{\rm R}(E_{\rm tot}-E_{\rm S}^{(1)})\)</span> is the number of states available to the reservoir, when the system is having the energy <span class="math notranslate nohighlight">\(E_{\rm S}^{1}\)</span>.</p>
<p>We can now rewrite the above equation in term of the entropy using <span class="math notranslate nohighlight">\(W=\exp(S/k_B)\)</span> such that</p>
<p><span class="math">\begin{equation}
\frac{W_{\rm R}(E_{\rm tot}-E_{\rm S}^{(1)})}{W_{\rm R}(E_{\rm tot}-E_{\rm S}^{(2)})}=\frac{\exp(S_{\rm R}(E_{\rm tot}-E_{\rm S}^{(1)})/k_B)}{\exp(S_{\rm R}(E_{\rm tot}-E_{\rm S}^{(2)})/k_B)}
\end{equation}</span></p>
<p>We may now expand the entropy to first order</p>
<p><span class="math">\begin{equation}
S_{\rm R}(E_{\rm tot}-E_{\rm S})\approx S_{\rm R}(E_{\rm tot})-\frac{\partial S_{\rm R}}{\partial E}E_{\rm S}
\end{equation}</span></p>
<p>considering that <span class="math notranslate nohighlight">\(E_{\rm S}\)</span> is only a very tiny as compared to the total energy of the reservoir. Using the thermodynamic identity that</p>
<p><span class="math">\begin{equation}
\frac{\partial S_{\rm R}}{\partial E}|_{V,N}=\frac{1}{T}
\end{equation}</span></p>
<p>we finally find</p>
<p><span class="math">\begin{equation}
\frac{p(E_s^{(1)})}{p(E_s^{(2)})}=\frac{\exp(-E_{\rm S}^{(1)}/k_BT)}{\exp(-E_{\rm S}^{(2)}/k_BT)}
\end{equation}</span></p>
<p>which corresponds to the ratio of two Boltzmann distributions</p>
<p><span class="math">\begin{equation}
p(E_{\rm S}^{(i)})=\frac{1}{Z}\exp\left (- \frac{E_{\rm S}^{(i)}}{k_B T}\right )
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(Z\)</span> is the previously mentioned normalization factor, which is called <em>partition function</em>.</p>
<div class="admonition note">
<p class="admonition-title"><strong>Example: Boltzmann Distribution is the Maximum Entropy Distribution in Which the Average Energy is Prescribed as a Constraint</strong></p>
<p>We can also obtain the Boltzmann distribution from the Shannon entropy by contraining the Shannon entropy. With only the normalization as a constraint in entropy maximization, we obtained equal likely microsates. If we now constrain the mean energy <span class="math notranslate nohighlight">\(\langle E\rangle\)</span> of the system we obtain a distribution which maximizes the entropy under this condition. The mean energy is given by</p>
<p><span class="math">\begin{equation}
\langle E\rangle=\sum_{i} E_{i} p_{i}
\end{equation}</span></p>
<p>such that we can add another constraint to our augmented entropy. We now have a Lagragian multilier <span class="math notranslate nohighlight">\(\lambda\)</span> for the normalization of the probability and a second one <span class="math notranslate nohighlight">\(\beta\)</span> which is multilolied by the energy constraint.</p>
<p><span class="math">\begin{equation}
S^{\prime}=-\sum_{i} p_{i} \ln p_{i}-\lambda\left(\sum_{i} p_{i}-1\right)-\beta\left(\sum_{i} p_{i} E_{i}-\langle E\rangle\right)
\end{equation}</span></p>
<p>Taking the derivative</p>
<p><span class="math">\begin{equation}
\frac{\partial S_{\prime}}{\partial p_{i}}=0=-\ln p_{i}-1-\lambda-\beta E_i
\end{equation}</span></p>
<p>results in</p>
<p><span class="math">\begin{equation}
p_{i}=\mathrm{e}^{-1-\lambda-\beta E_{i}}
\end{equation}</span></p>
<p>and together with the normalization condition <span class="math notranslate nohighlight">\(\sum p_i=1\)</span> finally</p>
<p><span class="math">\begin{equation}
\mathrm{e}^{-1-\lambda}=\frac{1}{\sum_{i} \mathrm{e}^{-\beta E_{i}}}
\end{equation}</span></p>
<p>where we already recognize that we can replace the prefactor <span class="math notranslate nohighlight">\(\mathrm{e}^{-1-\lambda}\)</span> by <span class="math notranslate nohighlight">\(1/Z\)</span> with <span class="math notranslate nohighlight">\(Z\)</span> being the partition function</p>
<p><span class="math">\begin{equation}
Z=\sum_{i} \mathrm{e}^{-\beta E_{l}}
\end{equation}</span></p>
<p>Overall this therefore leads to the Boltzmann distribution</p>
<p><span class="math">\begin{equation}
p_{i}=\frac{\mathrm{e}^{-\beta E_{i}}}{\sum_{i} \mathrm{e}^{-\beta E_{i}}}
\end{equation}</span></p>
<p>which is quite interesting. We have just fixed the mean energy of the system and maximized the entropy. The Boltzmann distribution is therefore the probability distribution which maximizes the entropy under as little as possible additional information (just the mean energy).</p>
<p>The only thing that is missing in the above formula is an expression for the value of <span class="math notranslate nohighlight">\(\beta\)</span>, the Lagrange multiplier. This can be obtained when knowning the mean energy. In thermal euilibrium, this mean energy can be obtained from the equipartition theorem.</p>
</div>
</div>
</div>
<div class="section" id="When-a-Macrostate-is-a-Microstate">
<h2>When a Macrostate is a Microstate<a class="headerlink" href="#When-a-Macrostate-is-a-Microstate" title="Permalink to this headline">¶</a></h2>
<p>In practice, we are often interested in the likelihood that the system a state that is described by some macroscopic parameter <span class="math notranslate nohighlight">\(X\)</span> that we can measure. For example, for a DNA molecule inside a cell, an interesting quantity, which can be measured using fluorescent markers, is the distance R between two sites on the DNA chain. Repeated measurements of <span class="math notranslate nohighlight">\(R\)</span> can to construct the probability distribution p(R).</p>
<p>In general, the probability of the macrostate <span class="math notranslate nohighlight">\(X\)</span> is given by the sum of probabilities of all the microstates of the system that adopt the specified value <span class="math notranslate nohighlight">\(X\)</span>,</p>
<p><span class="math">\begin{equation}
p(X)=\sum_{i_{X}} p_{i}=\sum_{i_{X}} \frac{1}{Z} \mathrm{e}^{-\beta E_{i}}
\end{equation}</span></p>
<p>For the DNA example, the sum in the above equation would run over only those microstates <span class="math notranslate nohighlight">\(i_X\)</span> that have the prescribed distance between the two labeled sites on the polymer, e.g. <span class="math notranslate nohighlight">\(X=R\)</span>. Using the basic relation between the partition function and the free energy, <span class="math notranslate nohighlight">\(G = −k_BT \ln(Z)\)</span>, we can express the probability of the macrostate X as</p>
<p><span class="math">\begin{equation}
p(X)=\frac{1}{Z} \mathrm{e}^{-\beta G(X)}
\end{equation}</span></p>
<p>where</p>
<p><span class="math">\begin{equation}
G(X)=-k_{\mathrm{B}} T \ln \left(\sum_{i_{X}} \mathrm{e}^{-\beta E_{i}}\right)
\end{equation}</span></p>
<p>is the free energy of the macrostate <span class="math notranslate nohighlight">\(X\)</span>. Note that the formula for <span class="math notranslate nohighlight">\(p(X)\)</span> is identical to the Boltzmann formula for the probability of a microstate, with the energy of the microstate replaced by the free energy of the macrostate. Note that the sum on the right side of the last equation is not the partition function <span class="math notranslate nohighlight">\(Z\)</span> but that of the subensemble of microstates fulfilling the condition X, i.e. <span class="math notranslate nohighlight">\(Z_X\)</span>. Similarly, when writing down the states and weights for the macrostates
<span class="math notranslate nohighlight">\(X\)</span>, the energy is replaced by the free energy, as shown in in the figure below. In this sense, one person’s macrostate is truly another person’s microstate.</p>
<p><img alt="micro_macro" class="bg-primary mb-1 no-scaled-link" src="../../_images/micro_macro.png" style="width: 500px;"/></p>
</div>
<div class="section" id="Equipartition">
<h2>Equipartition<a class="headerlink" href="#Equipartition" title="Permalink to this headline">¶</a></h2>
<p>In the previous section, we showed how the probability distribution for a system with average energy <span class="math notranslate nohighlight">\(\langle E \rangle\)</span> could be guessed by using the principle of maximum entropy. However, to finish that calculation, we need to determine the meaning and significance of the Lagrange multiplier <span class="math notranslate nohighlight">\(\beta\)</span>.</p>
<p>To obtain the lagrange parameter we just consider one degree of freedom for a monoatomic gas. This degree of freedom is the kinetic energy of one atom along the x-direction, which is given by</p>
<p><span class="math">\begin{equation}
E=\frac{p_{X}^{2}}{2 m}
\end{equation}</span></p>
<p>According to the Boltzmann distribution, the probability to find an atom with a certain momentum is given by</p>
<p><span class="math">\begin{equation}
P\left(p_{x}\right)=\frac{\mathrm{e}^{-\beta\left(p_{x}^{2} / 2 m\right)}}{\sum_{\text {states }} \mathrm{e}^{-\beta\left(p_{x}^{2} / 2 m\right)}}
\end{equation}</span></p>
<p>We can thus calculate the mean energy by summing up (or integrating when going to continuous states) over alls possible momenta</p>
<p><span class="math">\begin{equation}
\sum_{\text {states }} \rightarrow \int_{-\infty}^{\infty} \mathrm{d} p_{x}
\end{equation}</span></p>
<p>This yields</p>
<p><span class="math">\begin{equation}
\int_{-\infty}^{\infty} \mathrm{e}^{-\beta p_{x}^{2} / 2 m} \mathrm{~d} p_{x}=\sqrt{\frac{2 m \pi}{\beta}}
\end{equation}</span></p>
<p>To obtain the value of <span class="math notranslate nohighlight">\(\beta\)</span> we now constrain the mean energy to the value given by the equiparition principle</p>
<p><span class="math">\begin{equation}
\langle E\rangle=\frac{1}{2} k_{\mathrm{B}} T
\end{equation}</span></p>
<p>This is the mean energy per degree of freedom. One can show that each degree of freedom, independent of the object (atom, colloid, parking car) is carrying this mean energy. It provides actually our measure of temperature.</p>
<p>Using the momentum to calculate the mean energy we write down</p>
<p><span class="math">\begin{equation}
\langle E\rangle=\frac{\int_{-\infty}^{\infty} \frac{p_{X}^{2}}{2 m} \mathrm{e}^{-\beta\left(p_{x}^{2} / 2 m\right)} \mathrm{d} p_{x}}{\sqrt{2 m \pi / \beta}}
\end{equation}</span></p>
<p>which can be slightly simplified</p>
<p><span class="math">\begin{equation}
\langle E\rangle=\frac{\alpha^{3 / 2}}{\beta \sqrt{\pi}}\left(-\frac{\partial}{\partial \alpha}\right) \int_{-\infty}^{\infty} \mathrm{e}^{-\alpha p_{x}^{2}} \mathrm{~d} p_{x}
\end{equation}</span></p>
<p>and calculated via some tricks valid for integrals over Gaussian functions. This finally leads us to the result that the value of the Lagrangian multiplier must be</p>
<p><span class="math">\begin{equation}
\beta=\frac{1}{k_{\mathrm{B}} T}
\end{equation}</span></p>
<p>Equipartition is useful in many ways. One of them is the field of force measurements using optical tweezers.</p>
<p><img alt="tweezers" class="bg-primary mb-1 no-scaled-link" src="../../_images/tweezers.png" style="width: 800px;"/></p>
<script type="application/vnd.jupyter.widget-state+json">
{"state": {}, "version_major": 2, "version_minor": 0}
</script></div>
</div>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          
          <a class="prev-page" href="../L1/2_Thermodynamics_Statistics.html">
              <svg><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Thermodynamics and Statistical Physics Revisited</div>
                
              </div>
            </a>
        </div>

        <div class="related-information">
              Copyright &#169; 2021, Frank Cichos |
            Last updated on Oct 13, 2021. |
            Built with <a href="https://www.sphinx-doc.org/">Sphinx</a>
              and
              <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
              <a href="https://github.com/pradyunsg/furo">Furo theme</a>. |
            <a class="muted-link" href="../../_sources/notebooks/L2/3_Statistical_Physics_Definitions.ipynb.txt"
               rel="nofollow">
              Show Source
            </a>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            Contents
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Statistical Physics Definitions</a><ul>
<li><a class="reference internal" href="#Entropy">Entropy</a><ul>
<li><a class="reference internal" href="#Entropy-Definition-by-Boltzmann">Entropy Definition by Boltzmann</a></li>
<li><a class="reference internal" href="#Shannon-Entropy">Shannon Entropy</a></li>
</ul>
</li>
<li><a class="reference internal" href="#Boltzmann-Distribution">Boltzmann Distribution</a><ul>
<li><a class="reference internal" href="#Mean-Energy">Mean Energy</a></li>
<li><a class="reference internal" href="#Free-energy">Free energy</a></li>
<li><a class="reference internal" href="#Deriving-the-Boltzmann-Distribution">Deriving the Boltzmann Distribution</a></li>
</ul>
</li>
<li><a class="reference internal" href="#When-a-Macrostate-is-a-Microstate">When a Macrostate is a Microstate</a></li>
<li><a class="reference internal" href="#Equipartition">Equipartition</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/scripts/main.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"TeX": {"equationNumbers": {"autoNumber": "AMS", "useLabelIds": true}}, "tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    </body>
</html>